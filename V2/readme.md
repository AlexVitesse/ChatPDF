# Sistema de QA para Documentos con LangChain y Groq

Este proyecto implementa un sistema de preguntas y respuestas sobre documentos utilizando LangChain, Groq y tÃ©cnicas avanzadas de recuperaciÃ³n de informaciÃ³n. El sistema puede procesar documentos PDF (como "Los Miserables" en el ejemplo) y responder preguntas sobre su contenido.

## ğŸ“‹ Requerimientos

### ğŸ”§ Dependencias principales

Instala las siguientes bibliotecas con pip:

```bash
pip install langchain langchain-community chromadb pymupdf fastembed rank_bm25 groq
ğŸ“š Bibliotecas requeridas
langchain: Framework principal para construir cadenas de procesamiento

langchain-community: Integraciones con modelos y herramientas de la comunidad

chromadb: Base de datos vectorial para almacenar embeddings

pymupdf: Para procesamiento de archivos PDF

fastembed: Embeddings rÃ¡pidos y eficientes

rank_bm25: ImplementaciÃ³n del algoritmo BM25 para recuperaciÃ³n de informaciÃ³n

groq: Para acceder a los modelos de Groq

ğŸ”‘ Requisitos adicionales
API Key de Groq: Necesitas una clave API de Groq para usar sus modelos LLM. Puedes obtenerla en Groq Cloud.

Documento PDF: Por defecto el sistema busca un archivo llamado src/Los-miserables.pdf. Puedes:

Colocar tu propio PDF en esa ruta

Modificar el cÃ³digo para apuntar a tu archivo

ğŸ› ï¸ ConfiguraciÃ³n
Crea un archivo .env en el directorio raÃ­z con tu API key de Groq:

GROQ_API_KEY=tu_api_key_aquÃ­
Opcionalmente puedes configurar:

Modelo LLM (por defecto usa meta-llama/llama-4-maverick-17b-128e-instruct)

ParÃ¡metros de temperatura

ConfiguraciÃ³n de los chunks (tamaÃ±o y solapamiento)

ğŸš€ Uso
Ejecuta el script principal y sigue las instrucciones:

La primera vez que se ejecute, procesarÃ¡ el PDF y crearÃ¡ una base de datos vectorial (esto puede tomar varios minutos dependiendo del tamaÃ±o del documento).

En ejecuciones posteriores, cargarÃ¡ la base vectorial existente para mayor velocidad.

Ingresa tus preguntas sobre el documento cuando se te solicite.

Escribe "salir" para terminar la sesiÃ³n.

ğŸ§  Funcionamiento interno
El sistema utiliza:

Ensemble Retriever: Combina:

Vector Similarity (ChromaDB con embeddings)

BM25: Algoritmo tradicional de recuperaciÃ³n de informaciÃ³n

Modelo LLM de Groq: Para generar respuestas precisas basadas en el contexto recuperado

Procesamiento de documentos:

DivisiÃ³n en chunks con solapamiento

Embeddings con el modelo all-MiniLM-L6-v2

Persistencia en ChromaDB para reutilizaciÃ³n

ğŸ“„ Estructura de archivos
/proyecto
â”‚   README.md
â”‚   main.py                 # Script principal
â”‚   .env                    # ConfiguraciÃ³n de entorno
â”‚
â””â”€â”€â”€src/
    â”‚   Los-miserables.pdf  # Documento de ejemplo (puede ser reemplazado)
    â”‚
â””â”€â”€â”€chroma_db_dir/          # Base de datos vectorial (se crea automÃ¡ticamente)
ğŸ’¡ Mejoras posibles
AÃ±adir soporte para mÃºltiples formatos de documento (Word, HTML, etc.)

Implementar una interfaz web o de chat

AÃ±adir cachÃ© para respuestas frecuentes

Permitir mÃºltiples documentos/conocimientos

pero dame todo en formato md
markdown
# Sistema de QA para Documentos con LangChain y Groq

Este proyecto implementa un sistema de preguntas y respuestas sobre documentos utilizando LangChain, Groq y tÃ©cnicas avanzadas de recuperaciÃ³n de informaciÃ³n.

## ğŸ“‹ Requisitos TÃ©cnicos

### ğŸ”§ Dependencias Principales

```bash
pip install langchain langchain-community chromadb pymupdf fastembed rank_bm25 groq python-dotenv
ğŸ“š Bibliotecas Requeridas
Biblioteca	VersiÃ³n Recomendada	DescripciÃ³n
langchain	>=0.1.0	Framework principal
langchain-community	>=0.0.1	Integraciones comunitarias
chromadb	>=0.4.0	Base de datos vectorial
pymupdf	>=1.22.0	Procesamiento de PDFs
fastembed	>=0.2.0	Embeddings eficientes
rank_bm25	>=0.2.1	Algoritmo BM25
groq	>=0.3.0	Cliente para Groq API
python-dotenv	>=1.0.0	Manejo de variables de entorno
âš™ï¸ ConfiguraciÃ³n Inicial
Archivo .env:

GROQ_API_KEY=tu_api_key_aqui
MODEL_NAME=meta-llama/llama-4-maverick-17b-128e-instruct
Estructura de Directorios:

/proyecto
â”œâ”€â”€ src/
â”‚   â””â”€â”€ Los-miserables.pdf  # Documento de ejemplo
â”œâ”€â”€ chroma_db_dir/         # Base vectorial (auto-generado)
â”œâ”€â”€ .env                   # ConfiguraciÃ³n
â””â”€â”€ main.py                # Script principal
ğŸ› ï¸ Funcionalidades Clave
Procesamiento de Documentos
Soporte para archivos PDF

DivisiÃ³n inteligente de texto con:

Chunk size: 1000 caracteres

Overlap: 250 caracteres

Embeddings con all-MiniLM-L6-v2

Sistema de RecuperaciÃ³n
Ensemble Retriever que combina:

40% Vector Similarity (ChromaDB)

60% BM25

BÃºsqueda en top 10 chunks relevantes

Modelo de Lenguaje
Modelo: llama-4-maverick-17b-128e-instruct

Proveedor: Groq Cloud

Temperatura: 0.6 (balance creatividad/precisiÃ³n)

ğŸš€ CÃ³mo Usar
Primera EjecuciÃ³n:

bash
python main.py
ProcesarÃ¡ el PDF y crearÃ¡ la base vectorial

Ejecuciones Posteriores:

CargarÃ¡ la base vectorial existente

Modo interactivo para hacer preguntas

Comandos:

Ingresa tu pregunta

Escribe salir para terminar

ğŸ” Ejemplo de Uso
python
# Importaciones principales
from langchain.chat_models import ChatOpenAI
from pathlib import Path
from langchain_community.vectorstores import Chroma
from langchain.retrievers import BM25Retriever, EnsembleRetriever
import os
from dotenv import load_dotenv

load_dotenv()  # Carga variables de .env

# ConfiguraciÃ³n LLM
llm = ChatOpenAI(
    model_name=os.getenv("MODEL_NAME"),
    openai_api_base="https://api.groq.com/openai/v1",
    temperature=0.6,
    openai_api_key=os.getenv("GROQ_API_KEY")
)
ğŸ“Œ Notas Importantes
Requisitos de Hardware:

4GB RAM mÃ­nimo

2GB espacio en disco (para bases vectoriales grandes)

Limitaciones:

TamaÃ±o mÃ¡ximo de PDF: ~50MB

Soporte experimental para otros formatos

OptimizaciÃ³n:

Ajustar chunk_size segÃºn tipo de documento

Modificar weights del EnsembleRetriever para mejores resultados

ğŸ“„ Licencia
MIT License - Libre uso y modificaciÃ³n